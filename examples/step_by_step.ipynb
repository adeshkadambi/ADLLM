{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/adeshkadambi/WD_BLACK/PhD/test_folder/SCI06-5--16.MP4'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "video_dir = \"/media/adeshkadambi/WD_BLACK/PhD/test_folder/\"\n",
    "video_name = \"SCI06-5--16.MP4\"\n",
    "\n",
    "video_path = os.path.join(video_dir, video_name); video_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 4 frames\n",
      "Sampled indices: [0, 199, 399, 599]\n",
      "Total frames: 600\n"
     ]
    }
   ],
   "source": [
    "import video_utils as vu\n",
    "\n",
    "sampled_frames, sampled_indices, total_frames = vu.extract_frames(\n",
    "    path=os.path.join(video_dir, video_name),\n",
    "    sampling_method=vu.SamplingStrategy.UNIFORM,\n",
    "    num_frames=4,\n",
    "    save_frames=False,\n",
    ")\n",
    "print(f\"Extracted {len(sampled_frames)} frames\")\n",
    "print(f\"Sampled indices: {sampled_indices}\")\n",
    "print(f\"Total frames: {total_frames}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference import ADLClassifier\n",
    "\n",
    "clf = ADLClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 20:30:50,071 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2024-11-10 20:30:56,950 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2024-11-10 20:31:03,382 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2024-11-10 20:31:09,270 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2024-11-10 20:31:09,271 - inference - INFO - Frame analysis completed.\n"
     ]
    }
   ],
   "source": [
    "frame_descriptions = clf.analyse_frames(sampled_frames, sampled_indices, total_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 20:31:15,594 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2024-11-10 20:31:15,595 - inference - INFO - Context synthesis completed.\n"
     ]
    }
   ],
   "source": [
    "temporal_description = clf.synthesize_context(frame_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 20:31:22,497 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2024-11-10 20:31:22,498 - inference - INFO - ADL classification completed.\n"
     ]
    }
   ],
   "source": [
    "image_grid = clf._create_image_grid(sampled_frames)\n",
    "adl_prediction = clf.classify_adl(frame_descriptions, temporal_description, image_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a detailed description of the image, focusing on direct observations without interpretation:\n",
      "\n",
      "**Static Elements**\n",
      "\n",
      "* The room appears to be a small, cluttered space with white walls and a light-colored floor.\n",
      "* A wooden chair or table is visible in the top-left corner of the image.\n",
      "* Various objects are scattered around the room, including what looks like a red plastic basket, a black remote control, and some papers or documents.\n",
      "\n",
      "**Person's Position**\n",
      "\n",
      "* The person holding the camera is sitting on the floor with their legs stretched out to the right side of the image.\n",
      "* Their left hand is visible, holding a black remote control with multiple buttons.\n",
      "* Their right arm is not visible in this frame.\n",
      "* The person appears to be wearing a blue shirt and gray pants.\n",
      "\n",
      "**State of Objects**\n",
      "\n",
      "* The remote control is being held by the person's left hand, which is resting on their thigh.\n",
      "* There are no other objects actively being manipulated or moved within the frame.\n",
      "* The red plastic basket is positioned near the top-right corner of the image, but it does not appear to be in use.\n",
      "\n",
      "Overall, this single frame provides a snapshot of a cluttered room with a person sitting on the floor holding a remote control.\n",
      "\n",
      "\n",
      "=====================================\n",
      "Here is a detailed description of the image, focusing on direct observations without interpretation:\n",
      "\n",
      "**Static Elements**\n",
      "\n",
      "* The room appears to be cluttered with various items scattered around.\n",
      "* A wooden chair is visible in the bottom-left corner of the image.\n",
      "* In the background, there are several pieces of furniture and fixtures, including what looks like a TV stand or entertainment center.\n",
      "\n",
      "**Person's Position**\n",
      "\n",
      "* The person holding the camera is sitting on the floor, facing away from the viewer.\n",
      "* Their left hand is holding a remote control, while their right hand appears to be resting on their lap.\n",
      "* The person's body position is not clearly visible due to the first-person perspective of the image.\n",
      "\n",
      "**State of Objects**\n",
      "\n",
      "* The red plastic basket in front of the TV stand contains various items, including what looks like a green bag or towel.\n",
      "* A white plastic bag with red writing is hanging from the back of the chair in the bottom-left corner.\n",
      "* The remote control being held by the person appears to be a standard television remote.\n",
      "\n",
      "Overall, this image provides a snapshot of a cluttered room with various objects scattered around. The person holding the camera is sitting on the floor, facing away from the viewer, and holding a remote control.\n",
      "\n",
      "\n",
      "=====================================\n",
      "Here is a detailed description of the image, focusing on direct observations without interpretation:\n",
      "\n",
      "**Static Elements**\n",
      "\n",
      "* The room appears to be cluttered with various items scattered around.\n",
      "* A red plastic basket sits on the floor in the top center of the image, filled with an assortment of objects including what looks like a green cloth or bag and other indistinguishable items.\n",
      "* In the background, there is a wooden cabinet or bookshelf against the wall, partially visible.\n",
      "\n",
      "**Person's Position**\n",
      "\n",
      "* The person holding the camera is seated on a chair, facing away from the viewer. Their left hand holds a black remote control with white buttons.\n",
      "* Their right arm is not visible in this frame.\n",
      "* The person's legs are crossed at the ankles, and their feet are resting on a footrest or stool.\n",
      "\n",
      "**State of Objects**\n",
      "\n",
      "* The red basket appears to be stationary, sitting on the floor without any movement.\n",
      "* The black remote control is being held by the person's left hand, but it does not appear to be actively manipulated in this frame.\n",
      "* There are no other objects visible that seem to be in motion or being manipulated.\n",
      "\n",
      "\n",
      "=====================================\n",
      "Here is a detailed description of the image, focusing on direct observations without interpretation:\n",
      "\n",
      "**Static Elements**\n",
      "\n",
      "* The room appears to be cluttered with various items, including papers and other objects scattered around.\n",
      "* A wooden chair is visible in the top-left corner of the image.\n",
      "* In the background, there are several pieces of furniture or fixtures, but they are not clearly defined.\n",
      "\n",
      "**Person's Position**\n",
      "\n",
      "* The person holding the remote control is sitting on a chair, with their legs crossed.\n",
      "* Their left hand is holding the remote control, while their right hand appears to be resting on their lap.\n",
      "* The person's body position is partially visible, but it is unclear what they are doing or where they are looking.\n",
      "\n",
      "**State of Objects**\n",
      "\n",
      "* The remote control is being held in the person's left hand, with their fingers wrapped around it.\n",
      "* There does not appear to be any other object currently in the person's hands.\n",
      "* The chair and surrounding objects do not seem to be actively manipulated by the person.\n",
      "\n",
      "\n",
      "=====================================\n"
     ]
    }
   ],
   "source": [
    "for desc in frame_descriptions:\n",
    "    print(desc)\n",
    "    print(\"\\n\")\n",
    "    print(\"=====================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Constant Elements:**\n",
      "\n",
      "1. The room appears to be cluttered with various items scattered around.\n",
      "2. A wooden chair or table is visible in the top-left corner of the image (in frames 1, 3, and 4).\n",
      "3. Various objects are scattered around the room, including a red plastic basket, black remote control, papers or documents, green bag or towel, white plastic bag with red writing, and other indistinguishable items.\n",
      "4. The person holding the camera is sitting on the floor or chair, facing away from the viewer (in frames 1, 2, and 3).\n",
      "5. The person's left hand holds a black remote control with multiple buttons or standard television remote (in frames 1, 2, and 4).\n",
      "\n",
      "**Changes Between Frames:**\n",
      "\n",
      "1. Objects change position:\n",
      "\t* The red plastic basket is in the top-right corner of frame 1, near the TV stand in frame 2, sits on the floor in the top center of frame 3, and is not visible in frame 4.\n",
      "\t* The wooden chair or table is in the top-left corner of frames 1, 3, and 4, but its position changes slightly between frames.\n",
      "2. Hand positions change:\n",
      "\t* In frame 1, the person's left hand holds a black remote control with multiple buttons, while their right arm is not visible.\n",
      "\t* In frame 2, the person's left hand holds a standard television remote, and their right hand appears to be resting on their lap.\n",
      "\t* In frame 3, the person's left hand holds a black remote control with white buttons, but their right arm is not visible.\n",
      "\t* In frame 4, the person's left hand holds a remote control, while their right hand appears to be resting on their lap.\n",
      "3. New objects appear or disappear:\n",
      "\t* A green bag or towel is visible in the red plastic basket in frame 2 and is not mentioned in other frames.\n",
      "\t* A white plastic bag with red writing is hanging from the back of the chair in the bottom-left corner of frame 2, but it is not visible in other frames.\n"
     ]
    }
   ],
   "source": [
    "print(temporal_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"ADL\": \"LEISURE\",\n",
      "    \"Reasoning\": \"The person is sitting on the floor or chair, facing away from the viewer, and holding a remote control. This suggests they are engaged in leisure activity such as watching TV. The presence of various objects scattered around the room does not indicate any specific ADL being performed.\",\n",
      "    \"Activities\": \"Sitting on the floor/Chair, Holding Remote Control\",\n",
      "    \"Tags\": [\"Leisure\", \"Watching TV\", \"Remote Control\"],\n",
      "    \"Intermediate_Steps\": {\n",
      "        \"Environment_Analysis\": \"The room appears to be cluttered with various items scattered around. A wooden chair or table is visible in the top-left corner of the image.\",\n",
      "        \"ADL_Comparison\": \"The criteria for LEISURE are met as the person is engaged in a leisure activity (watching TV). The other categories do not fit as there is no indication of feeding, functional mobility, grooming and health management, communication management, home management, or meal preparation and cleanup.\",\n",
      "        \"OT_Discussion\": \"Occupational therapists agree that the person's actions are consistent with leisure activities. They note that the presence of a remote control suggests watching TV, which is a common leisure activity.\",\n",
      "        \"Expert_Evaluation\": \"Three occupational therapists evaluated the classification and agreed that LEISURE is the most appropriate category. They noted that the other categories do not fit as well.\",\n",
      "        \"Final_Verification\": \"The final classification aligns with all evidence. The person's actions, environment, and objects present suggest leisure activity.\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(adl_prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adl_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/adeshkadambi/WD_BLACK/PhD/test_folder/SCI02-2--9.MP4'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "video_dir = \"/media/adeshkadambi/WD_BLACK/PhD/test_folder/\"\n",
    "video_name = \"SCI02-2--9.MP4\"\n",
    "\n",
    "video_path = os.path.join(video_dir, video_name); video_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 4 frames\n",
      "Sampled indices: [0, 199, 399, 599]\n",
      "Total frames: 600\n"
     ]
    }
   ],
   "source": [
    "import video_utils as vu\n",
    "\n",
    "sampled_frames, sampled_indices, total_frames = vu.extract_frames(\n",
    "    path=os.path.join(video_dir, video_name),\n",
    "    sampling_method=vu.SamplingStrategy.UNIFORM,\n",
    "    num_frames=4,\n",
    "    save_frames=False,\n",
    ")\n",
    "print(f\"Extracted {len(sampled_frames)} frames\")\n",
    "print(f\"Sampled indices: {sampled_indices}\")\n",
    "print(f\"Total frames: {total_frames}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference import ADLClassifier\n",
    "\n",
    "clf = ADLClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-17 23:37:56,353 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 23:38:02,695 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 23:38:10,083 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 23:38:16,628 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 23:38:16,628 - inference - INFO - Frame analysis completed.\n"
     ]
    }
   ],
   "source": [
    "frame_descriptions = clf.analyse_frames(sampled_frames, sampled_indices, total_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 0: **Frame 0 Description**\n",
      "\n",
      "* **Room/Location:** The scene is set in a kitchen or dining area, featuring light-colored wood floors and a table with a glass top.\n",
      "* **Objects Present:**\n",
      "\t+ A white plate containing food\n",
      "\t+ A cup of coffee or tea on the right side of the plate\n",
      "\t+ A small bowl of fruit (likely grapes) on the left side of the plate\n",
      "\t+ A tablet or smartphone in a blue case on the table behind the plate\n",
      "* **Objects Being Actively Interacted With:** The person is actively interacting with the food on the plate, specifically holding a fork and bringing it to their mouth.\n",
      "* **Person's Actions:**\n",
      "\t+ The person is seated at the table, facing the camera.\n",
      "\t+ They are wearing dark-colored clothing and have short hair.\n",
      "\t+ Their right hand holds the fork, while their left hand supports the plate.\n",
      "\t+ The person appears to be eating a meal or snack.\n",
      "\n",
      "**Observations**\n",
      "\n",
      "* The room's decor suggests a casual, home environment.\n",
      "* The presence of a tablet or smartphone on the table implies that the person may have been using it before or during this activity.\n",
      "* The food on the plate is not clearly visible due to the angle and lighting in the frame.\n",
      "Frame 1: **Frame 199 Description**\n",
      "\n",
      "* **Room/Location:** The scene is set in a kitchen area, likely within a residential home.\n",
      "* **Objects Present:**\n",
      "\t+ A white plate containing food\n",
      "\t+ A cup of coffee or tea on the right side of the plate\n",
      "\t+ A partially visible object behind the plate, possibly a toaster or blender\n",
      "\t+ A wooden table with a glass top, featuring a floral patterned placemat underneath the plate\n",
      "* **Objects Being Actively Interacted With:** The person is actively interacting with the white plate and its contents.\n",
      "* **Person's Actions:**\n",
      "\t+ Holding a fork in their right hand, with the tines pointing towards the food on the plate\n",
      "\t+ Bringing the fork to their mouth, indicating they are eating or about to eat\n",
      "\n",
      "**Observations**\n",
      "\n",
      "The person appears to be enjoying a meal at home, possibly during breakfast or brunch. The presence of a cup of coffee or tea suggests that this may be part of a morning routine. The partially visible object behind the plate could indicate additional kitchen appliances or utensils nearby. Overall, the scene depicts a casual and intimate moment in the person's daily life.\n",
      "Frame 2: **Frame 399 Description**\n",
      "\n",
      "* **Room/Location:** The scene is set in a kitchen area, likely within a residential home.\n",
      "* **Objects Present:**\n",
      "\t+ A white plate containing food\n",
      "\t+ A fork on the left side of the plate\n",
      "\t+ A knife on the right side of the plate\n",
      "\t+ A bowl of fruit or salad to the left of the plate\n",
      "\t+ A cup or mug with a beverage, possibly coffee or tea, to the right of the plate\n",
      "* **Objects Being Actively Interacted With:** The person is actively interacting with the fork and knife on the plate.\n",
      "* **Person's Actions:**\n",
      "\t+ Holding the fork in their left hand\n",
      "\t+ Using the fork to pick up food from the plate\n",
      "\t+ Moving the fork towards their mouth, indicating they are eating\n",
      "\n",
      "**Observations**\n",
      "\n",
      "* The person appears to be seated at a table or countertop, with their back to the camera.\n",
      "* The kitchen area is well-lit, suggesting natural light sources or artificial lighting.\n",
      "* The presence of a cup or mug suggests that the person may have had a beverage before starting to eat.\n",
      "\n",
      "**Conclusion**\n",
      "\n",
      "Based on the visual data from this frame, it can be inferred that the person is engaged in eating an meal at home. The objects present and the person's actions suggest a casual dining setting, possibly during breakfast or lunchtime.\n",
      "Frame 3: **Frame 599 Description**\n",
      "\n",
      "* **Room/Location:** The scene is set in a kitchen area, likely within a residential home.\n",
      "* **Objects Present:**\n",
      "\t+ A white plate containing food\n",
      "\t+ A cup of coffee or tea\n",
      "\t+ A bowl of fruit (grapes and pineapple)\n",
      "\t+ A tablet with a blue case on the table\n",
      "* **Objects Being Actively Interacted With:** The person is actively interacting with the food on the plate, specifically using a fork to pick up an item.\n",
      "* **Person's Actions:**\n",
      "\t+ Holding a fork in their right hand\n",
      "\t+ Picking up an item from the plate\n",
      "\t+ Bringing the fork towards their mouth\n",
      "\n",
      "**Observations and Rationale**\n",
      "\n",
      "The person is seated at a table, likely eating breakfast or a snack. The presence of a cup of coffee or tea suggests that they may be enjoying a morning meal. The bowl of fruit on the table indicates that healthy options are available for consumption.\n",
      "\n",
      "The person's actions suggest that they are actively engaged in eating and drinking, using utensils to manipulate food items. This behavior is consistent with typical human activity during meals.\n",
      "\n",
      "Overall, this frame provides insight into the person's daily life and habits, specifically their eating patterns and preferences.\n"
     ]
    }
   ],
   "source": [
    "for idx, frame in enumerate(frame_descriptions):\n",
    "    print(f\"Frame {idx}: {frame}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporal_description = clf.synthesize_context(frame_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-17 23:38:21,221 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 23:38:21,221 - inference - INFO - ADL classification completed.\n"
     ]
    }
   ],
   "source": [
    "image_grid = clf._create_image_grid(sampled_frames)\n",
    "adl_prediction = clf.classify_adl(frame_descriptions, image_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(temporal_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ \n",
      "\"Activity\": \"EATING\", \n",
      "\"Tags\": [\"eating\", \"food\", \"plate\", \"fork\", \"knife\", \"cup\", \"beverage\"], \n",
      "\"Reasoning\": [\"The person is actively interacting with the food on the plate, specifically using a fork to pick up an item.\", \"The presence of a cup or mug suggests that the person may have had a beverage before starting to eat.\", \"The objects present and the person's actions suggest a casual dining setting, possibly during breakfast or lunchtime.\"] \n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(adl_prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adl_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

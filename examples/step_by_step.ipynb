{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/adeshkadambi/WD_BLACK/PhD/test_folder/SCI06-5--16.MP4'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "video_dir = \"/media/adeshkadambi/WD_BLACK/PhD/test_folder/\"\n",
    "video_name = \"SCI06-5--16.MP4\"\n",
    "\n",
    "video_path = os.path.join(video_dir, video_name); video_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 4 frames\n",
      "Sampled indices: [0, 199, 399, 599]\n",
      "Total frames: 600\n"
     ]
    }
   ],
   "source": [
    "import video_utils as vu\n",
    "\n",
    "sampled_frames, sampled_indices, total_frames = vu.extract_frames(\n",
    "    path=os.path.join(video_dir, video_name),\n",
    "    sampling_method=vu.SamplingStrategy.UNIFORM,\n",
    "    num_frames=4,\n",
    "    save_frames=False,\n",
    ")\n",
    "print(f\"Extracted {len(sampled_frames)} frames\")\n",
    "print(f\"Sampled indices: {sampled_indices}\")\n",
    "print(f\"Total frames: {total_frames}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference import ADLClassifier\n",
    "\n",
    "clf = ADLClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 22:57:44,899 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2024-11-10 22:57:51,546 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2024-11-10 22:57:58,683 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2024-11-10 22:58:06,039 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2024-11-10 22:58:06,040 - inference - INFO - Frame analysis completed.\n"
     ]
    }
   ],
   "source": [
    "frame_descriptions = clf.analyse_frames(sampled_frames, sampled_indices, total_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 22:58:10,176 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2024-11-10 22:58:10,177 - inference - INFO - Context synthesis completed.\n"
     ]
    }
   ],
   "source": [
    "temporal_description = clf.synthesize_context(frame_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 22:58:16,160 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2024-11-10 22:58:16,161 - inference - INFO - ADL classification completed.\n"
     ]
    }
   ],
   "source": [
    "image_grid = clf._create_image_grid(sampled_frames)\n",
    "adl_prediction = clf.classify_adl(frame_descriptions, temporal_description, image_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a detailed description of the image, focusing on direct observations without interpretation:\n",
      "\n",
      "**Static Elements**\n",
      "\n",
      "* The room appears to be a small, cluttered space with white walls and a light-colored floor.\n",
      "* A wooden chair is visible in the top-left corner of the frame, with papers or documents stacked on its seat.\n",
      "* In the background, there are several pieces of furniture, including a red plastic crate, a brown cabinet, and a metal table leg.\n",
      "* The floor is littered with various objects, such as a blue bag, a white box, and a black cord.\n",
      "\n",
      "**Person's Position**\n",
      "\n",
      "* The person holding the camera is sitting on the floor, facing downwards towards their lap.\n",
      "* Their left hand is visible, holding a remote control or similar device.\n",
      "* Their right arm is not visible in this frame.\n",
      "* The person appears to be wearing a blue shirt and gray pants.\n",
      "\n",
      "**State of Objects**\n",
      "\n",
      "* The remote control or device being held by the person's left hand is black with white buttons.\n",
      "* It is positioned directly in front of their face, suggesting that they are actively using it.\n",
      "* There are no other objects visible in close proximity to the person's hands or body.\n",
      "* The papers or documents on the chair appear to be stacked neatly, but there is no indication of whether they are being actively manipulated.\n",
      "\n",
      "Overall, this frame provides a glimpse into a cluttered and potentially disorganized space, with the person holding a remote control or device as their primary focus.\n",
      "\n",
      "\n",
      "=====================================\n",
      "Here is a detailed description of the image, focusing on direct observations without interpretation:\n",
      "\n",
      "**Static Elements**\n",
      "\n",
      "* The room appears to be a living area or kitchen, with white walls and a tiled floor.\n",
      "* A wooden cabinet is visible in the top-right corner of the frame.\n",
      "* A red plastic basket filled with various items sits on the floor near the cabinet.\n",
      "\n",
      "**Person's Position**\n",
      "\n",
      "* The person holding the camera is seated on a chair, facing away from the viewer.\n",
      "* Their left hand holds a black remote control, while their right hand grasps a blue object that is not clearly visible.\n",
      "* The person's body position is partially obscured by the chair and other objects in the room.\n",
      "\n",
      "**State of Objects**\n",
      "\n",
      "* The red basket contains various items, including what appears to be a green bag or cloth, but its contents are unclear due to the angle of the camera.\n",
      "* A white plastic bag with red writing on it hangs from the back of the cabinet.\n",
      "* Other objects in the room include a TV stand, a bookshelf, and several pieces of furniture that are not clearly identifiable.\n",
      "\n",
      "Overall, this frame provides a glimpse into the person's living space, but without more context or additional frames, it is difficult to interpret the purpose or significance of the objects present.\n",
      "\n",
      "\n",
      "=====================================\n",
      "Here is a detailed description of the image, focusing solely on direct observations without interpretation:\n",
      "\n",
      "**Static Elements**\n",
      "\n",
      "* The room appears to be cluttered with various items scattered around.\n",
      "* A red plastic basket filled with papers and other miscellaneous items sits on the floor in front of a wooden cabinet.\n",
      "* A black metal chair with torn fabric is positioned to the right of the person's legs.\n",
      "* A white wall is visible behind the chair, suggesting that the room may be small or compact.\n",
      "\n",
      "**Person's Position**\n",
      "\n",
      "* The person is seated in the chair, facing away from the camera and towards the left side of the image.\n",
      "* Their hands are not visible, but their arms appear to be resting on their thighs.\n",
      "* A white bandage or cast covers part of their right wrist, indicating that they may have an injury or medical condition affecting their hand.\n",
      "\n",
      "**State of Objects**\n",
      "\n",
      "* The red basket appears to be empty, with no objects visible inside it.\n",
      "* The black chair has torn fabric on its seat and backrest, suggesting that it may be worn out or damaged.\n",
      "* A white wall is visible behind the chair, providing a neutral background for the scene.\n",
      "\n",
      "Overall, this image provides a glimpse into a cluttered room with a person seated in a damaged chair, surrounded by various objects. The presence of a bandage on their wrist suggests that they may have an injury or medical condition affecting their hand.\n",
      "\n",
      "\n",
      "=====================================\n",
      "Here is a detailed description of the image, focusing on direct observations without interpretation:\n",
      "\n",
      "**Static Elements**\n",
      "\n",
      "* The room appears to be cluttered with various items scattered around.\n",
      "* A red plastic basket sits on the floor in the top center of the frame.\n",
      "* To the left of the basket, there are several pieces of paper or documents stacked haphazardly.\n",
      "* On the right side of the image, a black metal chair is visible, partially obscured by other objects.\n",
      "\n",
      "**Person's Position**\n",
      "\n",
      "* The person holding the remote control is sitting on the floor with their legs bent at an angle.\n",
      "* Their left hand grasps the remote control, while their right hand holds a beige cloth or towel.\n",
      "* The person's body position is not fully visible due to the camera angle and perspective.\n",
      "\n",
      "**State of Objects**\n",
      "\n",
      "* The red plastic basket contains some green items that are partially visible.\n",
      "* The papers or documents on the floor appear to be disorganized and scattered randomly.\n",
      "* The black metal chair seems to be in a stable position, but its exact orientation is unclear due to the clutter surrounding it.\n",
      "* The remote control is being actively manipulated by the person's left hand, which is holding it firmly.\n",
      "\n",
      "In summary, this frame shows a person sitting on the floor with their legs bent, holding a remote control and a beige cloth or towel. The room is cluttered with various objects, including a red plastic basket, papers or documents, and a black metal chair.\n",
      "\n",
      "\n",
      "=====================================\n"
     ]
    }
   ],
   "source": [
    "for desc in frame_descriptions:\n",
    "    print(desc)\n",
    "    print(\"\\n\")\n",
    "    print(\"=====================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Constant Elements:**\n",
      "\n",
      "1. White walls in all frames\n",
      "2. Cluttered rooms with various items scattered around\n",
      "3. Presence of furniture such as chairs, cabinets, and tables\n",
      "4. Red plastic baskets or containers in multiple frames\n",
      "5. Black remote controls or similar devices held by the person's left hand\n",
      "\n",
      "**Changes Between Frames:**\n",
      "\n",
      "1. Person's position changes:\n",
      "\t* Frame 1: Sitting on floor facing downwards\n",
      "\t* Frame 2: Seated on chair facing away from viewer\n",
      "\t* Frame 3: Seated in damaged chair facing away from camera\n",
      "\t* Frame 4: Sitting on floor with legs bent at an angle\n",
      "2. Hand positions change:\n",
      "\t* Frame 1: Left hand holds remote control, right arm not visible\n",
      "\t* Frame 2: Left hand holds black remote control, right hand grasps blue object\n",
      "\t* Frame 3: Hands not visible, but arms appear to be resting on thighs\n",
      "\t* Frame 4: Left hand holds remote control, right hand holds beige cloth or towel\n",
      "3. New objects appear or disappear:\n",
      "\t* Blue bag in Frame 1 is not present in other frames\n",
      "\t* White box and black cord in Frame 1 are not mentioned in other frames\n",
      "\t* Green items in red plastic basket in Frame 4 are not visible in other frames\n"
     ]
    }
   ],
   "source": [
    "print(temporal_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"ADL\": \"LEISURE\",\n",
      "    \"Reasoning\": \"The person is holding a remote control in their left hand and a beige cloth or towel in their right hand, indicating they are engaged in leisure activities. The cluttered room with various items scattered around also supports this classification.\",\n",
      "    \"Activities\": \"holding a remote control, sitting on the floor with legs bent at an angle\",\n",
      "    \"Tags\": [\"remote control\", \"beige cloth or towel\", \"cluttered room\"],\n",
      "    \"Intermediate_Steps\": {\n",
      "        \"Environment_Analysis\": \"The environment is cluttered with various items scattered around, and there are red plastic baskets or containers in multiple frames.\",\n",
      "        \"ADL_Comparison\": \"Leisure activities require sustained engagement with leisure objects, which is observed in the person holding a remote control. Other ADL categories do not fit as well.\",\n",
      "        \"OT_Discussion\": \"Occupational therapists agreed that the classification of LEISURE was most appropriate based on the evidence.\",\n",
      "        \"Expert_Evaluation\": \"Three occupational therapists evaluated the classification and found it to be accurate.\",\n",
      "        \"Final_Verification\": \"The final classification aligns with all evidence, including the person's sustained engagement with leisure objects.\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(adl_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adl_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

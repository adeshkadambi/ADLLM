{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/adeshkadambi/WD_BLACK/PhD/test_folder/SCI02-2--9.MP4'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "video_dir = \"/media/adeshkadambi/WD_BLACK/PhD/test_folder/\"\n",
    "video_name = \"SCI02-2--9.MP4\"\n",
    "\n",
    "video_path = os.path.join(video_dir, video_name); video_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 4 frames\n",
      "Sampled indices: [0, 199, 399, 599]\n",
      "Total frames: 600\n"
     ]
    }
   ],
   "source": [
    "import video_utils as vu\n",
    "\n",
    "sampled_frames, sampled_indices, total_frames = vu.extract_frames(\n",
    "    path=os.path.join(video_dir, video_name),\n",
    "    sampling_method=vu.SamplingStrategy.UNIFORM,\n",
    "    num_frames=4,\n",
    "    save_frames=False,\n",
    ")\n",
    "print(f\"Extracted {len(sampled_frames)} frames\")\n",
    "print(f\"Sampled indices: {sampled_indices}\")\n",
    "print(f\"Total frames: {total_frames}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference import ADLClassifier\n",
    "\n",
    "clf = ADLClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 18:51:48,349 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 18:51:55,503 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 18:52:04,262 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 18:52:10,958 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 18:52:10,958 - inference - INFO - Frame analysis completed.\n"
     ]
    }
   ],
   "source": [
    "frame_descriptions = clf.analyse_frames(sampled_frames, sampled_indices, total_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 18:52:14,105 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 18:52:14,106 - inference - INFO - Context synthesis completed.\n"
     ]
    }
   ],
   "source": [
    "temporal_description = clf.synthesize_context(frame_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 18:52:20,820 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 18:52:20,821 - inference - INFO - ADL classification completed.\n"
     ]
    }
   ],
   "source": [
    "image_grid = clf._create_image_grid(sampled_frames)\n",
    "adl_prediction = clf.classify_adl(frame_descriptions, temporal_description, image_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a detailed description of the image, focusing on direct observations without interpretation:\n",
      "\n",
      "**Static Elements**\n",
      "\n",
      "* The room appears to be a dining area or kitchen, with a wooden table and chair visible.\n",
      "* A blue tablet case is placed on the table, accompanied by an open tablet displaying a video recording interface.\n",
      "* A white coffee mug containing a light-brown liquid sits next to the tablet.\n",
      "\n",
      "**Person's Position**\n",
      "\n",
      "* The person is seated at the table, facing away from the camera.\n",
      "* Their right hand holds a fork with a piece of food speared on it, while their left hand rests on the table.\n",
      "* The person's body is partially visible, wearing dark-colored clothing and leaning slightly forward towards the plate.\n",
      "\n",
      "**State of Objects**\n",
      "\n",
      "* A white plate in front of the person contains a variety of foods, including poached eggs, bacon, cherry tomatoes, toast, and fruit salad.\n",
      "* The fork held by the person's right hand is positioned near their mouth, suggesting they are actively eating.\n",
      "* The coffee mug next to the tablet appears to be filled with a beverage, but its contents are not clearly visible.\n",
      "\n",
      "Overall, this frame captures a moment in time where the person is engaged in eating breakfast or brunch at a table in a dining area or kitchen.\n",
      "\n",
      "\n",
      "=====================================\n",
      "Here is a detailed description of the image, focusing on direct observations without interpretation:\n",
      "\n",
      "**Static Elements**\n",
      "\n",
      "* The room appears to be a dining area or kitchen, with a wooden table and chairs visible.\n",
      "* A blue tablet case is placed on the table, accompanied by a small bowl of fruit (likely grapes) in front of it.\n",
      "* A white plate with a floral pattern sits on top of a placemat, containing a breakfast meal consisting of poached eggs, bacon, toast, cherry tomatoes, and a cup of coffee or tea.\n",
      "\n",
      "**Person's Position**\n",
      "\n",
      "* The person is seated at the table, facing away from the camera, wearing dark blue sweatpants and a navy blue long-sleeved shirt.\n",
      "* Their right hand holds a fork, with their wrist bent downward as if they are about to take a bite.\n",
      "* Their left arm rests on the edge of the table, with their elbow slightly bent.\n",
      "\n",
      "**State of Objects**\n",
      "\n",
      "* The plate of food is positioned in front of the person, with the cup of coffee or tea placed above it.\n",
      "* The blue tablet case and bowl of fruit are situated behind the plate, out of reach but still visible.\n",
      "* The placemat under the plate features a floral pattern, adding a decorative touch to the table setting.\n",
      "\n",
      "Overall, this image captures a moment in time where the person is preparing to eat their breakfast meal, with various objects arranged around them on the table.\n",
      "\n",
      "\n",
      "=====================================\n",
      "The image shows a first-person perspective video captured using a head-mounted GoPro camera, with the subject sitting at a table. The following is a detailed description of the static elements and the person's position within the single frame:\n",
      "\n",
      "**Static Elements**\n",
      "\n",
      "*   Room/location visible: A dining room or kitchen area.\n",
      "*   Furniture or fixtures present:\n",
      "    *   A wooden chair to the left of the image, with its back facing the camera.\n",
      "    *   A tablecloth covering a rectangular table in front of the subject.\n",
      "    *   A blue tablet on the table, positioned near the top-left corner of the frame.\n",
      "    *   A white mug filled with coffee or tea on the right side of the plate.\n",
      "*   Objects visible in frame:\n",
      "    *   A white plate containing food and utensils.\n",
      "    *   A fork placed on the left side of the plate.\n",
      "    *   A knife positioned on the right side of the plate.\n",
      "\n",
      "**Person's Position**\n",
      "\n",
      "*   Hand position: The subject holds a mug with their right hand, while their left hand is not visible in the frame.\n",
      "*   Any object currently in hand: The mug filled with coffee or tea.\n",
      "*   Body position (if visible): Only the upper body and head of the subject are visible within the frame.\n",
      "\n",
      "**State of Objects**\n",
      "\n",
      "*   Position of objects relative to person:\n",
      "    *   The plate is centered on the table, directly in front of the subject's lap.\n",
      "    *   The mug is held by the subject's right hand, positioned near their mouth.\n",
      "    *   The fork and knife are placed on either side of the plate, with the fork on the left and the knife on the right.\n",
      "*   Whether objects are being actively manipulated: Yes, the subject is actively holding the mug in their right hand.\n",
      "\n",
      "\n",
      "=====================================\n",
      "Here is a detailed description of the image, focusing on direct observations without interpretation:\n",
      "\n",
      "**Static Elements**\n",
      "\n",
      "* The room appears to be a dining area or kitchen, with a wooden table and chairs visible.\n",
      "* A blue tablet case is placed on the table, accompanied by a white mug filled with coffee or tea.\n",
      "* A bowl of fruit, likely containing grapes and pineapple chunks, sits next to the plate.\n",
      "\n",
      "**Person's Position**\n",
      "\n",
      "* The person is seated at the table, wearing a dark-colored sweatshirt.\n",
      "* Their right hand holds a spoon, while their left hand grasps a white mug.\n",
      "* The person's body is partially visible, with their torso facing towards the camera and their legs extending out of frame to the right.\n",
      "\n",
      "**State of Objects**\n",
      "\n",
      "* The plate in front of the person contains a breakfast meal consisting of poached eggs, bacon or ham, cherry tomatoes, toast, and fruit.\n",
      "* The spoon held by the person's right hand is positioned near the top-left corner of the plate.\n",
      "* The white mug in their left hand appears to be filled with coffee or tea, as evidenced by its light brown color.\n",
      "\n",
      "In summary, this frame captures a person seated at a table, enjoying a breakfast meal while holding a spoon and sipping from a mug.\n",
      "\n",
      "\n",
      "=====================================\n"
     ]
    }
   ],
   "source": [
    "for desc in frame_descriptions:\n",
    "    print(desc)\n",
    "    print(\"\\n\")\n",
    "    print(\"=====================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Constant Elements:**\n",
      "\n",
      "* The room is consistently described as a dining area or kitchen.\n",
      "* A wooden table is present in all frames.\n",
      "* A blue tablet case is placed on the table in all but one frame (Frame 3).\n",
      "* A white plate with food and utensils is present in all frames.\n",
      "* A cup of coffee or tea is visible in most frames, either as a separate object or part of the breakfast meal.\n",
      "\n",
      "**Changes Between Frames:**\n",
      "\n",
      "* Objects that change position:\n",
      "\t+ The person's hand positions change between frames (e.g., holding a fork, spoon, or mug).\n",
      "\t+ The location of the bowl of fruit changes between frames.\n",
      "* New objects appear or disappear:\n",
      "\t+ A white coffee mug containing a light-brown liquid appears in Frame 1 and disappears in Frame 2.\n",
      "\t+ A knife is present in Frame 3 but not in other frames.\n",
      "\t+ A placemat with a floral pattern is visible in Frame 2 but not in other frames.\n"
     ]
    }
   ],
   "source": [
    "print(temporal_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"ADL\": \"FEEDING\",\n",
      "    \"Reasoning\": \"The person is seated at a table, holding a spoon and sipping from a mug. The plate in front of them contains a breakfast meal consisting of poached eggs, bacon or ham, cherry tomatoes, toast, and fruit. This indicates that the person is actively engaged in eating.\",\n",
      "    \"Activities\": \"Seated at a table, holding a spoon, sipping from a mug, eating a breakfast meal\",\n",
      "    \"Tags\": [\"eating\", \"drinking\", \"breakfast meal\"],\n",
      "    \"Intermediate_Steps\": {\n",
      "        \"Environment_Analysis\": \"The room is consistently described as a dining area or kitchen. A wooden table is present in all frames.\",\n",
      "        \"ADL_Comparison\": \"The person's hand positions change between frames, indicating that they are actively manipulating objects (e.g., holding a fork, spoon, or mug). The location of the bowl of fruit changes between frames.\",\n",
      "        \"OT_Discussion\": \"Occupational therapists agreed that the primary activity is feeding, as it involves consuming food and drink. They noted that the person's hand positions and object interactions are consistent with eating.\",\n",
      "        \"Expert_Evaluation\": \"Three occupational therapists evaluated the classification and found no disagreements. They confirmed that the person is engaged in feeding activities.\",\n",
      "        \"Final_Verification\": \"The final classification aligns with all evidence, including the person's actions, object interactions, and environmental context.\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(adl_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adl_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
